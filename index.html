<!DOCTYPE HTML>
<html lang="en">

<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

  <title>Qinyuan (程沁源)</title>

  <meta name="author" content="Qinyuan Cheng">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/jpg" href="images/fudan_icon.jpg">

  <script type="text/javascript">

    function display(id) {
      var traget = document.getElementById(id);
      if (traget.style.display == "none") {
        traget.style.display = "";
      } else {
        traget.style.display = "none";
      }
    }  
  </script>

  <style>
    body {
      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, 'Open Sans', 'Helvetica Neue', sans-serif;
      font-size: 16px; /* 增加默认字体大小 */
      line-height: 1.6; /* 适当增加行高 */
    }
    
    p, li, a, strong {  /* 添加 br 标签 */
      font-size: 16px; /* 段落字体大小 */
    }
    
    papertitle {
      font-size: 18px; /* 论文标题字体大小 */
    }
    
    papertitle a {  /* 添加这条规则 */
      font-size: 18px;
    }
    
    heading {
      font-size: 22px; /* 标题字体大小 */
    }
    
    heading a {  /* 添加这条规则 */
      font-size: 22px;
    }
  </style>
</head>

<body>
  <table
    style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
    <tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr style="padding:0px">
                <td style="padding:2.5%;width:63%;vertical-align:middle">
                  <p style="text-align:center">
                    <name>Qinyuan Cheng<img style="max-height:70px;vertical-align: middle;"
                        src="images/chinese_name.png" style="max-height:70px" /></name>
                  </p>
                  <p>Hi! I am a final year PhD student at Fudan University, advised by Prof. <a
                      href="https://xpqiu.github.io/">Xipeng Qiu</a>. 
                      I received my bachelor's degree from Sun Yat-sen University, advised by Prof. <a href="https://scholar.google.com.sg/citations?user=9LkhGDgAAAAJ&hl=zh-CN">Hanjiang Lai</a>. 
                      Previously, I interned at <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a> as a LLM researcher (2023.5-2025.1) and worked as a software development engineer intern at <a href="https://www.bytedance.com/en/">ByteDance</a> (2021.3-2021.8).
                  </p>
                  <p>
                    My research interests include post-training of large language models, including alignment (truthfullness and safety), reinforcement learning for LLMs (reasoning), multimodal large models (audio and vision), interpretability (mechanistic interpretability), and LLM-based agent systems (RAG and task-oriented dialogue systems).
                  </p>
                  <p>
                    I am now focused on building context intelligence, dedicated to bringing agent systems from the digital world into the real world.
                    <b>I am looking for highly self-motivated interns with expertise in large language models and multimodal (video, speech, image) generation and understanding.</b> For details, please contact me via email.
                    Reach out to me over email: <a
                      href="mailto:chengqy2019@foxmail.com">chengqy2019@foxmail.com</a>.
                  </p>
                  <p style="text-align:center">
                    <a href="/cv/cqy_cv_en.pdf">CV</a>
                    &nbsp/&nbsp
                    <a href="https://scholar.google.com/citations?user=nu_iPXAAAAAJ&hl=en">Google Scholar</a>
                    &nbsp/&nbsp
                    <a href="https://github.com/xiami2019">Github</a> &nbsp/&nbsp
                    <a href="https://x.com/cheng_qinyuan">X (twitter)</a> &nbsp/&nbsp
                    <a href="https://open-moss.com/">OpenMOSS</a>
                  </p>
                </td>
                <td style="padding:2.5%;width:40%;max-width:40%">
                  <a href="images/cqy.jpeg"><img style="width:80%;max-width:80" alt="profile photo"
                      src="images/cqy.jpeg" class="hoverZoomLink"></a>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>My Timeline</heading>
                  <ul style="line-height:1.8">
                    <!-- <a onclick="return display('old_news');"> ---- show more ----</a>
                    <div id="old_news" style="display: none;"> -->
                      <li> <b>[Oct. 2025]</b> We released <a href="https://github.com/OpenMOSS/MOSS-Speech">MOSS-Speech</a>, a speech-to-speech large language model that directly processes and generates speech without text intermediates, achieving state-of-the-art performance in spoken question answering and competitive text performance.</li>
                      <li> <b>[Sep. 2025]</b> One paper accepted to NeurIPS 2025!</li>
                      <li> <b>[June. 2025]</b> We released <a href="https://github.com/OpenMOSS/MOSS-TTSD">MOSS-TTSD</a>, a powerful TTS model for expressive spoken dialogue generation, and <a href="https://arxiv.org/pdf/2506.23325">XY-Tokenizer</a>, a novel low-bitrate codec that mitigates the conflict between semantic and acoustic capabilities.</li>
                      <li> <b>[May. 2025]</b> Five papers accepted to ACL 2025!</li>
                      <li> <b>[Feb. 2025]</b> New <a href="https://arxiv.org/pdf/2502.12215">Paper</a>: We revisited the test-time scaling capabilities of current open-source o1-like models and proposed a method combining parallel sampling and sequential revisions for better test-time scaling.</li>
                      <li> <b>[Jan. 2025]</b> We released <a href="https://github.com/OpenMOSS/SpeechGPT-2.0-preview">SpeechGPT-2.0-preview</a>, a GPT-4o-level, real-time spoken dialogue system.</li>
                      <li> <b>[Dec. 2024]</b> We released a survery paper about <a href="https://arxiv.org/pdf/2412.14135">o1's roadmap</a>, which has <a href="https://mp.weixin.qq.com/s/IOKFBgoWyietVe3NNNw9Hg">attracted widespread attention in the global AI community</a>. It's a teamwork with Zhiyuan Zeng, Zhangyue Yin and Bo Wang.</li>
                      <li> <b>[Sep. 2024]</b> Two papers accepted to COLING 2025!</li>
                      <li> <b>[Sep. 2024]</b> Four papers accepted to EMNLP 2024!</li>
                      <li> <b>[Jun. 2024]</b> We released <a href="https://github.com/sinwang20/SIUO">SIUO benchmark</a>. We first revealed that safe single-modal content may lead to unsafe responses when cross-modal inputs are involved.</li>
                      <li> <b>[Jun. 2024]</b> We proposed <a href="https://arxiv.org/pdf/2406.13990">Inference-Time Decontamination</a> to reuse leaked benchmarks for LLM evaluations.</li>
                      <li> <b>[Jun. 2024]</b> We proposed <a href="https://github.com/xiami2019/UAR">Unified Active Retrieval (UAR)</a> for RAG, a unified light-weight framework to address different active scenarios in RAG systems. This work is cited by <b>Google DeepMind</b></li>
                      <li> <b>[May. 2024]</b> One <a href="https://arxiv.org/pdf/2402.11251">paper</a> accepted to ACL 2024 (Findings) and one <a href="https://arxiv.org/pdf/2405.12939">paper</a> accepted to COLING 2024! </li>
                      <li> <b>[May. 2024]</b> One <a href="https://arxiv.org/pdf/2401.13275">paper</a> accepted to ICML 2024! We systematically studied truthfulness alignment (including SFT, DPO, BoN, PPO and HIR) of LLMs and proposed a framework to align LLMs' knowledge boundaries.</li>
                      <li> <b>[Mar. 2024]</b> Excited to announce <a href="https://open-moss.com/en/">OpenMOSS</a>! It's an open platform to share research from the MOSS team.</li>
                      <li> <b>[Jan. 2024]</b> I led a team to develop a LLM-based agent system in collaboration with <a href="https://www.honor.com/global/">HONOR</a>, which has been integrated into Magic OS 8.0 to help users create videos from their photo albums.</li>
                      <li> <b>[Dec. 2023]</b> I gave a talk on <a
                        href='https://www.bilibili.com/video/BV13Q4y1E7NJ/?spm_id_from=333.337.search-card.all.click&vd_source=6fbfb88b6c87aff21e195ccbd3e59fc9'>Evaluating Hallucinations in Chinese LLMs</a> at NICE.
                      </li>
                      <li> <b>[Oct. 2023]</b> We released <a href="https://github.com/OpenMOSS/HalluQA">HalluQA</a>, a high-quality adversarial hallucination benchmark for Chinese LLMs. This work is cited by <b>OpenAI</b> and <b>Google DeepMind</b></li>
                      <li> <b>[Aug. 2023]</b> I organized a <a
                        href="https://iacc.pazhoulab-huangpu.com/contestdetail?id=666abc727ff47da8cc804856&award=1,000,000">LLM post-training competition</a> about improving Chinese LLMs' intelligence, truthfulness, and safety (total prize of 1,000,000
                      RMB).</li>
                      <li> <b>[May. 2023]</b> I joined <a href="https://www.shlab.org.cn/">Shanghai AI Laboratory</a> and participated in the post-training of InternLM1. </li>
                      <li> <b>[May. 2023]</b> We proposed <a href="https://aclanthology.org/2023.findings-acl.707.pdf">CLAIF and CLHAIF</a>, the first work to use AI feedback to provide scalable training signals for Contrastive Learning and combine AI feedback with human feedback for futher improvement.</li>
                      <li> <b>[Feb. 2023]</b> We are excited to release <a
                        href="https://txsun1997.github.io/blogs/moss.html">MOSS</a>, a conversational language model. I was mainly responsible for data synthesis in the post-training stage, including multi-turn dialogue, safety alignment, honest alignment, etc.
                    </li>
                    <li> <b>[Nov. 2022]</b> One <a href="https://ojs.aaai.org/index.php/AAAI/article/view/26539">paper</a> accepted to AAAI 2023!</li>
                      <li> <b>[Oct. 2022]</b> My first paper is accepted to EMNLP 2022 (Findings)! We construct an <a href="https://aclanthology.org/2022.findings-emnlp.90.pdf">User Simulator</a> for task-oriented agents and use SFT + RL to fine-tune t5 models. Based on these, we propose an interactive evaluation framework for TOD agents and use reward-shaping to improve the quality of the generated responses.</li>
                      <li> <b>[Sep. 2022]</b> I gave a talk on diffusion models at FNLP. Document can be viewed
                      <a href='https://fudan-nlp.feishu.cn/docx/doxcnuc8DmpEH5V2PB1x8ydvKLc'>here</a>.
                      <li> <b>[Sep. 2021]</b> I joined the <a href="https://nlp.fudan.edu.cn/nlpen/main.htm">Fudan NLP Lab</a> at Fudan University as a master student. </li>
                    </div>
                  </ul>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Highlighted Papers</heading>
                  <p>
                    Full list of papers can be found at
                    <a href="https://scholar.google.com/citations?user=nu_iPXAAAAAJ&hl=en">Google Scholar</a>
                  </p>
                  <p>
                    (*: Equal contribution)
                  </p>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/say_idk.png' width="350">
                    </div>
                    <img src='images/say_idk.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://arxiv.org/abs/2401.13275">
                    <papertitle>Can AI Assistants Know What They Don't Know?</papertitle>
                  </a>
                  <br>
                  <strong>Qinyuan Cheng</strong>*, Tianxiang Sun*, Xiangyang Liu, Wenwei Zhang, Zhangyue Yin, Shimin Li,
                  Linyang Li, Zhengfu He, Kai Chen, Xipeng Qiu
                  <br>
                  ICML 2024 &nbsp
                  <br>
                  <a href="https://arxiv.org/pdf/2401.13275.pdf">pdf</a>
                  /
                  <a href="https://github.com/OpenMOSS/Say-I-Dont-Know">github</a>
                  /
                  <a href="https://open-moss.com/en/knowledge-boundary/">blog on OpenMOSS</a>
                  </p>
                  <p>
                    We ask the question "Can AI assistants know what they don't know and express them through natural
                    language?" To answer this question, we construct a model-specific "I don't know" (Idk) dataset for
                    an assistant, which contains its known and unknown questions, based on existing open-domain question
                    answering datasets. Then we design a series of truthfulness alignment methods to align the assistant with its corresponding Idk dataset and observe
                    whether it can refuse to answer its unknown questions after alignment.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/HalluQA.png' width="350">
                    </div>
                    <img src='images/HalluQA.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://arxiv.org/pdf/2310.03368">
                    <papertitle>Evaluating Hallucinations in Chinese Large Language Models
                    </papertitle>
                  </a>
                  <br>
                  <strong>Qinyuan Cheng</strong>, Tianxiang Sun, Wenwei Zhang, Siyin Wang, Xiangyang Liu, Mozhi Zhang, Junliang He, Mianqiu Huang, Zhangyue Yin, Kai Chen, Xipeng Qiu
                  <br>
                  Preprint 2023, cited by OpenAI and Google DeepMind &nbsp
                  <br>
                  <a href="https://arxiv.org/pdf/2310.03368">pdf</a>
                  /
                  <a href="https://github.com/OpenMOSS/HalluQA">github</a>
                  /
                  <a href="https://zhuanlan.zhihu.com/p/663070312">blog</a>
                  </p>
                  <p>
                    We introduce HalluQA, a benchmark with 450 adversarial questions to assess hallucinations in Chinese large language models, covering diverse domains and cultural contexts. It targets imitative falsehoods and factual errors, built using GLM-130B and ChatGPT, and evaluated automatically with GPT-4. Testing 24 models like ERNIE-Bot and Qwen, 18 scored below 50% non-hallucination rates, showing its difficulty. We analyze hallucination types, causes, and prioritization for different models.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/CLAIF.png' width="350">
                    </div>
                    <img src='images/CLAIF.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://aclanthology.org/2023.findings-acl.707.pdf">
                    <papertitle>Improving Contrastive Learning of Sentence Embeddings from AI Feedback
                    </papertitle>
                  </a>
                  <br>
                  <strong>Qinyuan Cheng</strong>, Xiaogui Yang, Tianxiang Sun, Linyang Li, Xipeng Qiu
                  <br>
                  ACL 2023 Findings &nbsp
                  <br>
                  <a href="https://aclanthology.org/2023.findings-acl.707.pdf">pdf</a>
                  /
                  <a href="https://github.com/xiami2019/CLAIF">github</a>
                  /
                  <a href="https://zhuanlan.zhihu.com/p/627201325">blog</a>
                  </p>
                  <p>
                    We propose Contrastive Learning of sentence embeddings from AI Feedback (CLAIF) to enhance contrastive learning in NLP. Unlike typical methods struggling with sample pair quality due to language's discrete nature, CLAIF uses AI feedback from large language models to create fine-grained similarity scores for sample pairs. Combining this with human feedback, it improves supervised contrastive learning. Experiments show CLAIF outperforms other methods on semantic textual similarity and transfer learning tasks.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/UAR_framework.png' width="350">
                    </div>
                    <img src='images/UAR_framework.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://aclanthology.org/2024.findings-emnlp.999.pdf">
                    <papertitle>Unified Active Retrieval for Retrieval Augmented Generation
                    </papertitle>
                  </a>
                  <br>
                  <strong>Qinyuan Cheng</strong>*, Xiaonan Li*, Shimin Li, Qin Zhu, Zhangyue Yin, Yunfan Shao, Linyang Li, Tianxiang Sun, Hang Yan, Xipeng Qiu
                  <br>
                  EMNLP 2024 Findings, cited by Google DeepMind &nbsp
                  <br>
                  <a href="https://aclanthology.org/2024.findings-emnlp.999.pdf">pdf</a>
                  /
                  <a href="https://github.com/xiami2019/UAR">github</a> /
                  <a href="https://mp.weixin.qq.com/s/4i5lWgTkp1GpsCJzdq6PEg">blog</a>
                  </p>
                  <p>
                    We introduce Unified Active Retrieval (UAR) to improve Retrieval-Augmented Generation (RAG) by addressing inefficiencies in existing active retrieval methods. UAR uses four orthogonal criteria as simple classification tasks for better retrieval timing, with minimal extra cost. Supported by UAR-Criteria, it handles diverse instructions effectively. Experiments show UAR outperforms others in accuracy and downstream tasks.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/User_simulator.png' width="350">
                    </div>
                    <img src='images/User_simulator.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://aclanthology.org/2022.findings-emnlp.90.pdf">
                    <papertitle>Is MultiWOZ a Solved Task? An Interactive TOD Evaluation Framework with User Simulator
                    </papertitle>
                  </a>
                  <br>
                  <strong>Qinyuan Cheng</strong>*, Linyang Li*, Guofeng Quan, Feng Gao, Xiaofeng Mou, Xipeng Qiu
                  <br>
                  EMNLP 2022 Findings &nbsp
                  <br>
                  <a href="https://aclanthology.org/2022.findings-emnlp.90.pdf">pdf</a>
                  /
                  <a href="https://github.com/xiami2019/User-Simulator">github</a>
                  </p>
                  <p>
                    We propose an interactive evaluation framework for Task-Oriented Dialogue (TOD) systems to address the policy mismatch in current methods, where user utterances don’t align with varied system responses. Our approach uses a goal-oriented user simulator built on pre-trained models to generate dialogues, introducing sentence- and session-level scores for fluency and coherence. Experiments show RL-based TOD systems trained with our simulator achieve 98% inform and success rates on MultiWOZ, with the new scores enhancing response quality assessment.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/SIUO.png' width="350">
                    </div>
                    <img src='images/SIUO.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://arxiv.org/pdf/2406.15279">
                    <papertitle>Cross-modality safety alignment
                    </papertitle>
                  </a>
                  <br>
                  Siyin Wang, Xingsong Ye, <strong>Qinyuan Cheng</strong>, Junwen Duan, Shimin Li, Jinlan Fu, Xipeng Qiu, Xuanjing Huang
                  <br>
                  NAACL 2025 Findings &nbsp
                  <br>
                  <a href="https://arxiv.org/pdf/2406.15279">pdf</a>
                  /
                  <a href="https://sinwang20.github.io/SIUO/">Project Page</a>
                  </p>
                  <p>
                    We present the Safe Inputs but Unsafe Output (SIUO) challenge to assess cross-modality safety in Artificial General Intelligence (AGI), where individually safe modalities can combine to produce unsafe outputs. Unlike prior studies on single-modality risks, SIUO targets complex interactions across 9 safety domains, including self-harm and privacy violations. Our benchmark tests reveal significant vulnerabilities in models like GPT-4V and LLaVA, highlighting their limitations in handling real-world scenarios safely.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:top">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/OthelloGPT.jpg' width="250">
                    </div>
                    <img src='images/OthelloGPT.jpg' width="250">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://arxiv.org/abs/2402.12201">
                    <papertitle>Dictionary Learning Improves Patch-Free Circuit Discovery in Mechanistic
                      Interpretability: A Case Study on Othello-GPT</papertitle>
                  </a>
                  <br>
                  Zhengfu He, Xuyang Ge, Qiong Tang, Tianxiang Sun, <strong>Qinyuan Cheng</strong>, Xipeng Qiu
                  <br>
                  <em>arXiv</em>, 2402.12201 &nbsp
                  <br>
                  <a href="https://arxiv.org/pdf/2402.12201.pdf">pdf</a>
                  /
                  <a href="https://open-moss.com/en/DictCircuits_Othello/">blog on OpenMOSS</a>
                  </p>
                  <p>
                    Sparse dictionary learning has been a rapidly growing technique in mechanistic interpretability to
                    attack superposition and extract more human-understandable features from model activations. We ask a
                    further question based on the extracted more monosemantic features: How do we recognize circuits
                    connecting the enormous amount of dictionary features? We propose a circuit discovery framework
                    alternative to activation patching.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Projects & Resources</heading>
                </td>
              </tr>
            </tbody>
          </table>
          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/moss-ttsd.png' width="350">
                    </div>
                    <img src='images/moss-ttsd.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://github.com/OpenMOSS/MOSS-TTSD">
                    <papertitle>MOSS-TTSD: Text to Spoken Dialogue Generation</papertitle>
                  </a>
                  <br>
                  </p>
                  <p>
                    MOSS-TTSD (text to spoken dialogue) is an open-source bilingual spoken dialogue synthesis model that supports both Chinese and English. It can transform dialogue scripts between two speakers into natural, expressive conversational speech. MOSS-TTSD supports voice cloning and long single-session speech generation, making it ideal for AI podcast production, interviews, and chats.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/sg2_arch.png' width="350">
                    </div>
                    <img src='images/sg2_arch.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://github.com/OpenMOSS/SpeechGPT-2.0-preview">
                    <papertitle>SpeechGPT-2.0-preview</papertitle>
                  </a>
                  <br>
                  Mainly responsible for text data curation (including pre-training and post-training).
                  </p>
                  <p>
                    We introduce SpeechGPT 2.0-preview, our first human-like real-time interaction system, trained on millions of hours of Chinese speech data. This end-to-end model offers low-latency, natural responses with human-like expressions, supporting interruptions and multi-style emotional control. It excels in role-playing, vocal talents like poetry and dialects, and integrates text capabilities for tool use and searches. Currently, it supports only Chinese, with no English training yet.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/o1_roadmap.png' width="350">
                    </div>
                    <img src='images/o1_roadmap.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p>
                  <a href="https://arxiv.org/pdf/2412.14135">
                    <papertitle>Survey Paper: Scaling of Search and Learning: A Roadmap to Reproduce o1
                      from Reinforcement Learning Perspective</papertitle>
                  </a>
                  <br>
                  First co-author.
                  </p>
                  <p>
                    We analyze OpenAI's o1, an AI milestone excelling in reasoning tasks, driven by reinforcement learning. Unlike alternatives like knowledge distillation, limited by teacher models, our roadmap focuses on four key components: policy initialization for human-like reasoning, reward design for effective guidance, search for high-quality solutions, and learning to enhance performance with more data and parameters. These elements highlight how learning and search power o1's success, influencing LLM development.
                  </p>
                </td>
              </tr>
              <tr onmouseout="malle_stop()" onmouseover="malle_start()">
                <td style="padding:20px;width:40%;vertical-align:middle">
                  <div class="one">
                    <div class="two" id='malle_image'>
                      <img src='images/moss.png' width="350">
                    </div>
                    <img src='images/moss.png' width="350">
                  </div>
                  <script type="text/javascript">
                    function malle_start() {
                      document.getElementById('malle_image').style.opacity = "1";
                    }

                    function malle_stop() {
                      document.getElementById('malle_image').style.opacity = "0";
                    }
                    malle_stop()
                  </script>
                </td>
                <td style="padding:20px;width:70%;vertical-align:top">
                  <p></p>
                  <a href="http://moss.fastnlp.top/">
                    <papertitle>MOSS: A Conversational Language Model</papertitle>
                  </a>
                  <br>
                  Mainly responsible for data synthesis in the post-training stage, including multi-turn dialogue, safety alignment, honest alignment.
                  </p>
                  <p>
                    MOSS is a conversational language model like ChatGPT. It is capable of following users' instructions
                    to perform various natural language tasks including question answering, generating text, summarzing
                    text, generating code, etc. MOSS is also able to challenge incorrect premises, and reject
                    inappropriate requests. Here is a brief <a
                      href="https://txsun1997.github.io/blogs/moss.html">introduction</a> to MOSS.
                  </p>
                </td>
              </tr>

            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:100%;vertical-align:middle">
                  <heading>Service</heading>
                  <p>
                    <b>Reviewer / Program Committee Member</b>
                  </p>
                  <ul>
                    <li> ACL (2023, 2024, 2025)</li>
                    <li> EMNLP (2022, 2023, 2024, 2025)</li>
                    <li> COLING (2025)</li>
                    <li> NAACL (2025)</li>
                    <li> ICML (2025)</li>
                    <li> ICLR (2025)</li>
                    <li> NeurIPS (2024, 2025)</li>
                    <li> AISTATS (2025)</li>
                  </ul>
                  <!-- <em>Will not submit papers or reviews to AAAI/IJCAI since 2022 :)</em> -->
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tbody>
              <tr>
                <td style="padding:20px;width:30%;vertical-align:middle">
                  <script type="text/javascript" id="clustrmaps" src="//clustrmaps.com/map_v2.js?d=N_G0i4IbHiu2LKpyE1m-HTt_N6fdM7m69dptQPOde5c&cl=ffffff&w=a"></script>
                </td>
              </tr>
            </tbody>
          </table>

          <table
            style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;">
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:center;font-size:small;">
                  Design and source code from <a style="font-size:small;" href="https://jonbarron.info">Jon Barron's
                    website</a>
                </p>
              </td>
            </tr>
          </table>
        </td>
      </tr>
  </table>
</body>

</html>
